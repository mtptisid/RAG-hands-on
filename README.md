# ðŸ§  RAG-hands-on

A practical hands-on repository for exploring Retrieval-Augmented Generation (RAG) using open-source tools and LLMs.

---

## âœ… What Has Been Done So Far

- âœ… Environment setup completed (`env-setup/`)
- âœ… Installed and configured necessary libraries (e.g., `transformers`, `faiss`, `langchain`, `llama-cpp`)
- âœ… Added quantized model support using `bitsandbytes` for 4-bit/8-bit loading
- âœ… Implemented document ingestion pipeline:
  - Document loading
  - Chunking using LangChain
  - Embedding generation with Hugging Face models
  - Vector store indexing (FAISS, Chroma)
- âœ… Created working RAG pipeline:
  - User query â†’ Retriever â†’ LLM â†’ Final answer
- âœ… Demonstrated Use Case 1: Simple RAG Q&A on sample data (LLaMA3 + A4000 GPU)
- âœ… Explored concepts like memory, temperature, top-p, and prompt engineering
- âœ… Drafted multi-step theoretical insights and notes
- âœ… Visualized working flow (text + image)

---

## ðŸ§­ What to Do Next

### ðŸ”§ 1. Modularize Codebase

- Convert exploratory notebooks/scripts into reusable Python modules:
  - `src/ingest.py`
  - `src/retrieve.py`
  - `src/generate.py`
- Add CLI interface for each stage (with `argparse`)

### ðŸ“¦ 2. Add `demo/` Folder for Full Pipeline

- Sample commands:
  ```bash
  python demo/ingest.py --input ./docs --output ./vectorstore
  python demo/query.py --question "What is RAG?" --model llama
